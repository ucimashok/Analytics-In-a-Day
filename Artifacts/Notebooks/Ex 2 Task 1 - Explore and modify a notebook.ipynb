{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {
        "microsoft": {
          "language": "scala"
        }
      },
      "source": [
        "%%spark\r\n",
        "\r\n",
        "// Set the path to read the WWI Sales files\r\n",
        "import org.apache.spark.sql.SparkSession\r\n",
        "\r\n",
        "// Set the path to the ADLS Gen2 account\r\n",
        "val adlsPath = \"abfss://wwi@azsynapseaiddl.dfs.core.windows.net\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "collapsed": true
      },
      "source": [
        "%%spark\r\n",
        "\r\n",
        "// Read the sales into a dataframe\r\n",
        "val sales = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"sep\", \"|\").load(s\"$adlsPath/factsale-csv/2012/Q4\")\r\n",
        "sales.show(5)\r\n",
        "sales.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "scala"
        },
        "collapsed": true
      },
      "source": [
        "%%spark\r\n",
        "\r\n",
        "// Import libraries for the SQL Analytics connector\r\n",
        "import com.microsoft.spark.sqlanalytics.utils.Constants\r\n",
        "import org.apache.spark.sql.SqlAnalyticsConnector._\r\n",
        "import org.apache.spark.sql.SaveMode\r\n",
        "\r\n",
        "// Set target table name\r\n",
        "var tableName = s\"SQLPool01.wwi_staging.Sale\"\r\n",
        "\r\n",
        "// Write the retrieved sales data into a staging table in Azure Synapse Analytics.\r\n",
        "sales.limit(10000).write.mode(SaveMode.Append).sqlanalytics(tableName, Constants.INTERNAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      },
      "source": [
        ""
      ]
    }
  ]
}